{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/home/duynguyenngoc/Desktop/project/analytic/dwh_analytic/')\n",
    "os.chdir('D:\\\\github\\\\analytic\\\\dwh_analytic\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dags.data_warehouse_prod.schema.init_data import InitData\n",
    "from dags.data_warehouse_prod.functions.db_connect import EngineConnect as DatabaseConnect\n",
    "from dags.data_warehouse_prod.settings import config\n",
    "db_connect = DatabaseConnect(uri = config.DWH_SQLALCHEMY_URI)\n",
    "db_connect.close()\n",
    "InitData().dim_project_variable(db_connect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "import pendulum\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "from dags.data_warehouse_prod.settings import config\n",
    "from dags.data_warehouse_prod.functions import function as func\n",
    "from dags.data_warehouse_prod.functions.db_connect import EngineConnect as DatabaseConnect\n",
    "from dags.data_warehouse_prod.functions import report as report_func\n",
    "from dags.data_warehouse_prod.schema.fact_document import FactDocumentModel\n",
    "from dags.data_warehouse_prod.schema.dim_field import DimFieldModel\n",
    "from dags.data_warehouse_prod.schema.fact_performance import FactPerformanceModel\n",
    "from dags.data_warehouse_prod.schema.fact_data_extraction import FactDataExtractionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *kwargs,\n",
    "        environment: str,\n",
    "        project_id: str,\n",
    "        uri: str,\n",
    "        database_name: str = 'elrond',\n",
    "        db: DatabaseConnect\n",
    "    ):\n",
    "        self.backup_dir = config.BACKUP_DIR\n",
    "        self.schema = config.DWH_ANALYTIC_SCHEMA\n",
    "        self.fact_document_table = config.DWH_FACT_DOCUMENT_TABLE\n",
    "        self.fact_performance_table = config.DWH_FACT_PERFORMANCE_TABLE\n",
    "        self.fact_data_extraction_table = config.DWH_FACT_DATA_EXTRACTION_TABLE\n",
    "        self.fact_report_table = config.DWH_FACT_REPORT_ETL_TABLE\n",
    "        self.dim_project_variable_table = config.DWH_DIM_PROJECT_VARIABLE_TABLE\n",
    "        self.dim_field_table = config.DWH_DIM_FIELD_TABLE\n",
    "        self.maxSevSelDelay = 20000\n",
    "        self.start = config.start\n",
    "        self.database_name  = config.ELROND_DATABASE\n",
    "        \n",
    "        self.db = db\n",
    "        self.environment = environment\n",
    "        self.project_id = project_id\n",
    "        \n",
    "        self.var_load = db.get_one_data_by_field_name_and_value('project_id',project_id,self.schema,self.dim_project_variable_table)\n",
    "        self.project_name = config.GDA_PROJECT_NAME\n",
    "        self.docs_collection_name = self.var_load['docs_collection_name']\n",
    "        self.trans_collection_name = self.var_load['trans_collection_name']\n",
    "        self.performance_collection_name = self.var_load['performance_collection_name']\n",
    "        self.field_collection_name = self.var_load['field_collection_name']\n",
    "        self.docs_query = self.var_load['docs_query']\n",
    "        self.trans_query = self.var_load['trans_query']\n",
    "        self.performance_query = self.var_load['performance_query']\n",
    "        self.field_query = self.var_load['field_query']\n",
    "        self.project_backup_dir = self.var_load['project_backup_dir']\n",
    "        self.project_docs_dir = self.var_load['project_docs_dir']\n",
    "        self.project_trans_dir = self.var_load['project_trans_dir']\n",
    "        self.project_performance_dir = self.var_load['project_performance_dir']\n",
    "        self.project_field_dir = self.var_load['project_field_dir']\n",
    "        self.backup_file_type = self.var_load['backup_file_type']\n",
    "        self.schedule_type = self.var_load['schedule_type']\n",
    "        self.schedule_date_key = self.var_load['schedule_date_key']\n",
    "        self.schedule_time_key = self.var_load['schedule_time_key']\n",
    "       \n",
    "        self.reports = []\n",
    "        self.document_key_checks = []\n",
    "        self.performance_key_checks = []\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                      [LOANDING DOCUMENTS & TRANS]\n",
    "#==================================================================================================================================\n",
    "    def get_docs_and_trans(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_docs = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_docs_dir \\\n",
    "                                        + self.project_id + self.backup_file_type, 'rb'))\n",
    "            obj_trans = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_trans_dir \\\n",
    "                                         + self.project_id + self.backup_file_type, 'rb'))\n",
    "        else: \n",
    "            obj_docs = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_docs_dir \\\n",
    "                                        + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "            obj_trans = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_trans_dir \\\n",
    "                                         + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_docs = [item for item in obj_docs]\n",
    "        data_trans = [item for item in obj_trans]\n",
    "        return data_docs, data_trans\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                            [LOANDING PERFORMANCE]\n",
    "#================================================================================================================================== \n",
    "    def get_performance(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_performance = pickle.load(open('dags/data_warehouse_prod/backup/' +self.project_backup_dir + \\\n",
    "                                               self.project_performance_dir + self.project_id + self.backup_file_type, 'rb'))\n",
    "        else:\n",
    "            obj_performance = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_performance_dir \\\n",
    "                                               + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_performance = [item for item in obj_performance]\n",
    "        return data_performance\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                 [LOANDING FIELDS]\n",
    "#==================================================================================================================================     \n",
    "    def get_field(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_load = pickle.load(open('dags/data_warehouse_prod/backup/' +self.project_backup_dir + self.project_field_dir \\\n",
    "                                        + self.project_id + self.backup_file_type, 'rb'))\n",
    "        else:\n",
    "            obj_load = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_field_dir \\\n",
    "                                        + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        obj_list = [item for item in obj_load]\n",
    "        return obj_list\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                      [CLEAN TASK]\n",
    "#================================================================================================================================== \n",
    "    def clean(self):\n",
    "        report = report_func.initial_report('clean', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development' or self.environment == 'production':\n",
    "                now = self.start - timedelta(days=1)\n",
    "                file_name = str(now.strftime(\"%Y-%m-%d\"))\n",
    "                list_path = [\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_docs_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_trans_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_performance_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_field_dir + file_name + self.backup_file_type,\n",
    "                ]\n",
    "                description = ''\n",
    "                for item in list_path:\n",
    "                    if os.path.exists(item):\n",
    "                        os.remove(item)\n",
    "                    else:\n",
    "                        description +=  ', ' + \"The {path} does not exist\".format(path = item)\n",
    "                report.description = description\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "\n",
    "#==================================================================================================================================\n",
    "#                                                                                                                 [BACKUP DOCUMENT]\n",
    "#================================================================================================================================== \n",
    "    def backup_docs(self):\n",
    "        report = report_func.initial_report('backup_docs', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_docs_dir \\\n",
    "                                           + self.project_id + self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/docs_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.docs_collection_name].find(self.query_docs)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_docs_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                       [BACKUP DOCUMENT TRANSFORM]\n",
    "#================================================================================================================================== \n",
    "    def backup_trans(self):\n",
    "        report = report_func.initial_report('backup_trans', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_trans_dir \\\n",
    "                                           + self.project_id+self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/trans_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.trans_collection_name].find(self.query_trans)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_trans_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type , 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                              [BACKUP PERFORMANCE]\n",
    "#================================================================================================================================== \n",
    "    def backup_performance(self):\n",
    "        report = report_func.initial_report('backup_performance', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir \\\n",
    "                                           + self.project_performance_dir + self.project_id + self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/performance_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.performance_collection_name].find(self.query_performance)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_performance_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                    [BACKUP FIELD]\n",
    "#================================================================================================================================== \n",
    "    def backup_field(self):\n",
    "        report = report_func.initial_report('backup_field', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_field_dir \\\n",
    "                                           + self.project_id + self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/field_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.performance_collection_name].find(self.query_field)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_performance_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [UPLOAD_REPORT]\n",
    "#==================================================================================================================================\n",
    "    def report(self):\n",
    "        report_func.upload_report(self.reports, self.schema, self.fact_etl_report, self.db)\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [UPLOAD_REPORT]\n",
    "#==================================================================================================================================\n",
    "    def dim_field(self):\n",
    "        report = report_func.initial_report('dim_field', self.project_id, self.schedule_type, self.schedule_date_key,\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            data_fields = self.get_field()\n",
    "            results = []\n",
    "            for data in data_fields:\n",
    "                _obj = DimFieldModel(\n",
    "                        field_key = func.bson_object_to_string(data['_id']),\n",
    "                        project_id = self.project_id,\n",
    "                        name = data['name'],\n",
    "                        control_type =  data['control_type'],\n",
    "                        default_value = data['default_value'],\n",
    "                        counted_character = data['counted_character'],\n",
    "                        is_sub_field = False,\n",
    "                    )\n",
    "                results.append(_obj)\n",
    "            report.status_code = 'PASSED'\n",
    "            self.db.update([item.__dict__ for item in results], self.schema, self.dim_field_table)\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [UPLOAD_REPORT]\n",
    "#==================================================================================================================================\n",
    "    def fact_document(self):\n",
    "        report = report_func.initial_report('fact_document', self.project_id, self.schedule_type, self.schedule_date_key,\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            datas = []\n",
    "            data_docs, data_trans = self.get_docs_and_trans()\n",
    "            list_created = [{'doc_id': func.bson_object_to_string(data['_id']), 'created_date': data['created_date']} \\\n",
    "                            for data in data_docs]\n",
    "            _id = self.db.get_max_id_table(schema = self.schema, table = self.fact_document_table, col = 'document_key')\n",
    "            if _id == None: _id = 1\n",
    "            else: _id+=1\n",
    "            for data in data_trans:\n",
    "                if len(data['records']) == 0:\n",
    "                    continue\n",
    "                created_date_utc_7  = func.created_date_of_docs_by_id(func.bson_object_to_string(data['doc_id']), \\\n",
    "                                                                      list_created) + datetime.timedelta(hours = 7)\n",
    "                last_modified_utc_7 = data['last_modified'] + datetime.timedelta(hours = 7)\n",
    "                import_date_key_utc_7, import_time_key_utc_7 = func.handle_date_to_date_and_time_id(created_date_utc_7)\n",
    "                export_date_key_utc_7, export_time_key_utc_7 = func.handle_date_to_date_and_time_id(last_modified_utc_7)\n",
    "                document_id = func.bson_object_to_string(data['doc_id'])\n",
    "                doc_set_id = func.bson_object_to_string(data['doc_set_id']),\n",
    "                _obj = FactDocumentModel(\n",
    "                    document_key = _id,\n",
    "                    ori_document_id = func.bson_object_to_string(data['_id']),\n",
    "                    project_id = self.project_id,\n",
    "                    document_id = document_id,\n",
    "                    doc_set_id =  doc_set_id,\n",
    "                    remark_code = None,\n",
    "                    remark_description = None,\n",
    "                    import_date_key = import_date_key_utc_7,\n",
    "                    import_time_key = import_time_key_utc_7,\n",
    "                    export_date_key = export_date_key_utc_7,\n",
    "                    export_time_key = export_time_key_utc_7,\n",
    "                    import_timestamp = created_date_utc_7,\n",
    "                    export_timestamp = last_modified_utc_7,\n",
    "                )\n",
    "                self.document_key_checks.append({'document_key': _id, 'document_id': document_id, 'doc_set_id': doc_set_id})\n",
    "                datas.append(_obj)\n",
    "                _id+=1\n",
    "            self.db.create([item.__dict__ for item in datas], self.schema, self.fact_document_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "    def get_document_key_by_document_id(self, document_id: str):\n",
    "        document_key = None\n",
    "        for item in self.document_key_checks:\n",
    "            if item['document_id'] == document_id:\n",
    "                document_key = item['document_key']\n",
    "                break\n",
    "        return document_key\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                [FACT PERFORMANCE]\n",
    "#==================================================================================================================================\n",
    "    def fact_performance(self):\n",
    "        report = report_func.initial_report('fact_performance', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            datas = []\n",
    "            data_performance = self.get_performance()\n",
    "            _id = self.db.get_max_id_table(schema = self.schema, table = self.fact_performance_table, col ='performance_key')\n",
    "            if _id == None: _id = 1\n",
    "            else: _id+=1\n",
    "            for performance in data_performance:\n",
    "                captured_date_timestamp_utc_7 = performance['time'] + datetime.timedelta(hours = 7)\n",
    "                document_key = self.get_document_key_by_document_id(performance['doc_id'])\n",
    "                obj_ = FactPerformanceModel(\n",
    "                        performance_key = _id,\n",
    "                        ori_performance_id = func.bson_object_to_string(performance['_id']),\n",
    "                        document_key = document_key,\n",
    "                        project_id = self.project_id,  \n",
    "                        group_id = performance['group_id'],  \n",
    "                        document_id = performance['doc_id'],  \n",
    "                        reworked = func.int_to_bool(performance['rework_count']),  \n",
    "                        work_type_key = func.get_working_type_id_by_name(performance['work_type']),  \n",
    "                        process_key = func.get_process_key_performance_gda(performance['type'], performance['task_def_key']),  \n",
    "                        number_of_record = performance['records'],\n",
    "                        number_of_item = performance['items'],  \n",
    "                        number_of_field = performance['fields'],\n",
    "                        number_of_character = performance['chars'],  \n",
    "                        user_name = performance['username'], \n",
    "                        ip = performance['ip'], \n",
    "                        captured_date_timestamp = captured_date_timestamp_utc_7,  \n",
    "                        captured_date_key = func.time_to_date_key(captured_date_timestamp_utc_7),  \n",
    "                        captured_time_key = func.time_to_time_key(captured_date_timestamp_utc_7),  \n",
    "                        total_time_second = performance['total_time']    \n",
    "                )\n",
    "                datas.append(obj_)\n",
    "                self.performance_key_checks.append({'performance_key': _id, 'user_name': performance['username'], \n",
    "                                                    'document_key': document_key, 'module_type': performance['type'], \n",
    "                                                    'task_def_key': performance['task_def_key']})\n",
    "                _id+=1\n",
    "            self.db.create([item.__dict__ for item in datas], self.schema, self.fact_performance_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "    def get_performance_key(self, document_key: str, user_name: str, module_type: str, task_def_key: str):\n",
    "        performance_key = None\n",
    "        for item in self.performance_key_checks:\n",
    "            if item['user_name'] == user_name and item['module_type'] == module_type \\\n",
    "            and item['document_key'] == document_key and item['task_def_key'] == task_def_key:\n",
    "                performance_key = item['performance_key']\n",
    "                break\n",
    "        return performance_key\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                            [FACT DATA EXTRACTION]\n",
    "#==================================================================================================================================\n",
    "    def fact_data_extract(self):\n",
    "        report = report_func.initial_report('fact_data_extraction', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            data_docs, data_trans = self.get_docs_and_trans()\n",
    "            col_ignores = ['ImagePath']\n",
    "            results = []                \n",
    "            for data in data_docs:\n",
    "                records = data['records']\n",
    "                document_id = func.bson_object_to_string(data['_id'])          \n",
    "                document_key = self.get_document_key_by_document_id(document_id)\n",
    "                doc_set_id = func.bson_object_to_string(data['doc_set_id'])\n",
    "                for i in range(len(records)):\n",
    "                    record_id = i+1\n",
    "                    record = records[i]\n",
    "                    for key, value in record.items():\n",
    "                        if key == 'keyed_data':\n",
    "                            for keyed_data in value:\n",
    "                                source = keyed_data['source']\n",
    "                                task_def_key = keyed_data['task_def_key']\n",
    "                                data_needed = keyed_data['data'][0].items()\n",
    "                                last_modified_utc_7 = keyed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                                user_name = keyed_data['keyer']\n",
    "                                performance_key = None\n",
    "                                if source != 'queue_transform' and task_def_key.startswith('Type'):\n",
    "                                    process_key = 3 # human input keyed_data kpi\n",
    "                                    performance_key = self.get_performance_key(document_key, user_name, 'keying', task_def_key) \n",
    "                                if source != 'queue_transform' and task_def_key == 'Verify_Hold_Type':\n",
    "                                    process_key = 12 # human check bad_image keyed_data not kpi                               \n",
    "                                elif source == 'queue_transform' and task_def_key.startswith('Type'):\n",
    "                                    process_key = 4 # 'machine save input keyed_data'\n",
    "                                elif source != 'queue_transform' and task_def_key.startswith('Proof'):\n",
    "                                    process_key = 5 # human qc input keyed_data' kpi\n",
    "                                    performance_key = self.get_performance_key(document_key, user_name, 'keying', task_def_key)                                  \n",
    "                                elif source == 'queue_transform' and task_def_key.startswith('Proof'):\n",
    "                                    process_key = 6 # 'machine save qc keyed_data'\n",
    "                                for field_name, field_value_dict in data_needed:\n",
    "                                    if field_name in col_ignores:\n",
    "                                        continue\n",
    "                                    _obj = FactDataExtractionModel(\n",
    "                                        document_key = document_key,\n",
    "                                        performance_key = performance_key,\n",
    "                                        ori_document_id = document_id,\n",
    "                                        project_id = self.project_id,\n",
    "                                        document_id = document_id,\n",
    "                                        doc_set_id =  doc_set_id,\n",
    "                                        record_id = record_id,\n",
    "                                        last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                        last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                        last_modified_timestamp = last_modified_utc_7,\n",
    "                                        user_name = user_name,\n",
    "                                        process_key = process_key,\n",
    "                                        field_name = field_name,\n",
    "                                        field_value = field_value_dict['text']\n",
    "                                    )\n",
    "                                    results.append(_obj)\n",
    "\n",
    "                        elif key == 'final_data':\n",
    "                            final_data = value[0]\n",
    "                            data_needed = final_data['data'][0].items()\n",
    "                            last_modified_utc_7 = final_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                            user_name = final_data['keyer']\n",
    "                            process_key = 10\n",
    "                            for field_name, field_value_dict in data_needed:\n",
    "                                if field_name in col_ignores:\n",
    "                                    continue\n",
    "                                _obj = FactDataExtractionModel(\n",
    "                                    document_key = document_key,\n",
    "                                    performance_key = None,\n",
    "                                    ori_document_id = document_id,\n",
    "                                    project_id = self.project_id,\n",
    "                                    document_id = document_id,\n",
    "                                    doc_set_id =  doc_set_id,\n",
    "                                    record_id = record_id,\n",
    "                                    last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                    last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                    last_modified_timestamp = last_modified_utc_7,\n",
    "                                    user_name = user_name,\n",
    "                                    process_key = process_key,\n",
    "                                    field_name = field_name,\n",
    "                                    field_value = field_value_dict['text']\n",
    "                                )\n",
    "                                results.append(_obj)\n",
    "\n",
    "                        elif key == 'qc_ed_data':\n",
    "                            qc_ed_data = value[0][0]\n",
    "                            if 'qc_fields_err' not in qc_ed_data.keys():\n",
    "                                continue\n",
    "                            qc_ed_data_err = qc_ed_data['qc_fields_err']\n",
    "                            data_needed = qc_ed_data_err[0].items()\n",
    "                            last_modified_utc_7 = qc_ed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                            user_name = qc_ed_data['keyer']\n",
    "                            process_key = 8\n",
    "                            performance_key = None\n",
    "                            performance_key = self.get_performance_key(document_key, user_name, 'qc', task_def_key) \n",
    "                            for field_name, field_value_dict in data_needed:\n",
    "                                if field_name in col_ignores:\n",
    "                                    continue\n",
    "                                _obj = FactDataExtractionModel(\n",
    "                                    document_key = document_key,\n",
    "                                    performance_key = performance_key,\n",
    "                                    ori_document_id = document_id,\n",
    "                                    project_id = self.project_id,\n",
    "                                    document_id = document_id,\n",
    "                                    doc_set_id =  doc_set_id,\n",
    "                                    record_id = record_id,\n",
    "                                    last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                    last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                    last_modified_timestamp = last_modified_utc_7,\n",
    "                                    user_name = user_name,\n",
    "                                    process_key = process_key,\n",
    "                                    field_name = field_name,\n",
    "                                    field_value = field_value_dict['text']\n",
    "                                )\n",
    "                                results.append(_obj)\n",
    "\n",
    "                        elif key == 'apr_ed_data':\n",
    "                            report.description = 'Not handle aprove qc data because not have sample data'\n",
    "                            \n",
    "            for data in data_trans:\n",
    "                trans_ignore = ['doc_id', 'doc_uri', 'fileName', 'fileName_Bad', 'filter_control', 'getBatchName', 'keyer', 'keyer_proof', 'keyer_type', 'FolderOutput', 'Image']\n",
    "                document_id = func.bson_object_to_string(data['doc_id'])\n",
    "                document_key = self.get_document_key_by_document_id(document_id)\n",
    "                ori_document_id = func.bson_object_to_string(data['_id'])\n",
    "                doc_set_id =  func.bson_object_to_string(data['doc_set_id'])\n",
    "                performance_key = None\n",
    "                records = data['records']\n",
    "                last_modified_utc_7 = data['last_modified'] + datetime.timedelta(hours = 7)\n",
    "                for i in range(len(records)):\n",
    "                    record = records[i]\n",
    "                    record_id = i + 1\n",
    "                    data_needed = record.items()\n",
    "                    for field_name, field_value in data_needed:\n",
    "                        if field_name in trans_ignore:\n",
    "                            continue\n",
    "                        _obj = FactDataExtractionModel(\n",
    "                            document_key = document_key,\n",
    "                            performance_key = performance_key,\n",
    "                            ori_document_id = ori_document_id,\n",
    "                            project_id = self.project_id,\n",
    "                            document_id = document_id,\n",
    "                            doc_set_id =  doc_set_id,\n",
    "                            record_id = record_id,\n",
    "                            last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                            last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                            last_modified_timestamp = last_modified_utc_7,\n",
    "                            user_name = None,\n",
    "                            process_key = 11,\n",
    "                            field_name = field_name,\n",
    "                            field_value = field_value\n",
    "                        )\n",
    "                        results.append(_obj)\n",
    "            self.db.create([item.__dict__ for item in results], self.schema, self.fact_data_extraction_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': ', The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/docs/2021-01-16.pickle '\n",
      "                'does not exist, The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/trans/2021-01-16.pickle '\n",
      "                'does not exist, The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/performance/2021-01-16.pickle '\n",
      "                'does not exist, The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/field/2021-01-16.pickle '\n",
      "                'does not exist',\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 5, 78290, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17345,\n",
      " 'job_name': 'clean',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.0}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 5, 85292, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17345,\n",
      " 'job_name': 'backup_docs',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 3.1014211177825928}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 8, 336667, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17348,\n",
      " 'job_name': 'backup_trans',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.36500000953674316}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 8, 732668, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17348,\n",
      " 'job_name': 'backup_performance',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.025008201599121094}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 8, 761669, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17348,\n",
      " 'job_name': 'backup_field',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.0039997100830078125}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 8, 766670, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17348,\n",
      " 'job_name': 'dim_field',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.2406597137451172}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 9, 8375, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 17349,\n",
      " 'job_name': 'fact_document',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 3.5119125843048096}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 12, 657279, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 173412,\n",
      " 'job_name': 'fact_performance',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 1.9507884979248047}\n",
      "{'description': '(psycopg2.errors.NotNullViolation) null value in column '\n",
      "                '\"project_name\" of relation \"fact_data_extraction\" violates '\n",
      "                'not-null constraint\\n'\n",
      "                'DETAIL:  Failing row contains (1, null, 2054, null, '\n",
      "                '5ff2df88474eb70010c1a1a5, 5e9e7ec598d753001b7efe6b, '\n",
      "                '5ff2df88474eb70010c1a1a5, 5ff2df88474eb70010c1a0e7, 1, '\n",
      "                '112525, 2021107, hangntt_2, 3, lfdNr, 121, 2021-01-07 '\n",
      "                '11:25:25.218).\\n'\n",
      "                '\\n'\n",
      "                '[SQL: INSERT INTO '\n",
      "                'dwh_development_analytic.fact_data_extraction (document_key, '\n",
      "                'performance_key, ori_document_id, project_id, document_id, '\n",
      "                'doc_set_id, record_id, last_modified_time_key, '\n",
      "                'last_modified_date_key, user_name, process_key, field_name, '\n",
      "                'field_value, last_modified_timestamp) VALUES '\n",
      "                '(%(document_key)s, %(performance_key)s, %(ori_document_id)s, '\n",
      "                '%(project_id)s, %(document_id)s, %(doc_set_id)s, '\n",
      "                '%(record_id)s, %(last_modified_time_key)s, '\n",
      "                '%(last_modified_date_key)s, %(user_name)s, %(process_key)s, '\n",
      "                '%(field_name)s, %(field_value)s, '\n",
      "                '%(last_modified_timestamp)s)]\\n'\n",
      "                \"[parameters: ({'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112525, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 3, \"\n",
      "                \"'field_name': 'lfdNr', 'field_value': '121', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 25, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112525, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 3, \"\n",
      "                \"'field_name': 'Bauherr', 'field_value': 'Koller Maria, \"\n",
      "                \"Hausfrau, Straubing, Inn. Frühlingsstraße, 2/II', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 25, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112525, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 3, \"\n",
      "                \"'field_name': 'Bauort', 'field_value': 'Alburg', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 25, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112525, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 3, \"\n",
      "                \"'field_name': 'Bauvorhaben', 'field_value': 'Wohnhaus', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 25, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112525, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 3, \"\n",
      "                \"'field_name': 'Note', 'field_value': '', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 25, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112527, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 4, \"\n",
      "                \"'field_name': 'lfdNr', 'field_value': '121', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 27, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112527, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 4, \"\n",
      "                \"'field_name': 'Bauherr', 'field_value': 'Koller Maria, \"\n",
      "                \"Hausfrau, Straubing, Inn. Frühlingsstraße, 2/II', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                \"25, 27, 218000)}, {'document_key': 2054.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ff2df88474eb70010c1a1a5', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff2df88474eb70010c1a1a5', 'doc_set_id': \"\n",
      "                \"'5ff2df88474eb70010c1a0e7', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 112527, 'last_modified_date_key': \"\n",
      "                \"2021107, 'user_name': 'hangntt_2', 'process_key': 4, \"\n",
      "                \"'field_name': 'Bauort', 'field_value': 'Alburg', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, \"\n",
      "                '25, 27, 218000)}  ... displaying 10 of 676857 total bound '\n",
      "                \"parameter sets ...  {'document_key': 2145.0, \"\n",
      "                \"'performance_key': None, 'ori_document_id': \"\n",
      "                \"'5ffc33b61b3ae8001e142f58', 'project_id': \"\n",
      "                \"'5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff5a00f474eb70010c28847', 'doc_set_id': \"\n",
      "                \"'5ff5a00e474eb70010c28789', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 181710, 'last_modified_date_key': \"\n",
      "                \"2021111, 'user_name': None, 'process_key': 11, 'field_name': \"\n",
      "                \"'Bauunternehmens', 'field_value': '', \"\n",
      "                \"'last_modified_timestamp': datetime.datetime(2021, 1, 11, 18, \"\n",
      "                \"17, 10, 693000)}, {'document_key': 2145.0, 'performance_key': \"\n",
      "                \"None, 'ori_document_id': '5ffc33b61b3ae8001e142f58', \"\n",
      "                \"'project_id': '5e9e7ec598d753001b7efe6b', 'document_id': \"\n",
      "                \"'5ff5a00f474eb70010c28847', 'doc_set_id': \"\n",
      "                \"'5ff5a00e474eb70010c28789', 'record_id': 1, \"\n",
      "                \"'last_modified_time_key': 181710, 'last_modified_date_key': \"\n",
      "                \"2021111, 'user_name': None, 'process_key': 11, 'field_name': \"\n",
      "                \"'plNr', 'field_value': '', 'last_modified_timestamp': \"\n",
      "                'datetime.datetime(2021, 1, 11, 18, 17, 10, 693000)})]\\n'\n",
      "                '(Background on this error at: http://sqlalche.me/e/13/gkpj)',\n",
      " 'executor_date_key': 2021118,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 18, 17, 34, 14, 611073, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 173414,\n",
      " 'job_name': 'fact_data_extraction',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'FAILED',\n",
      " 'total_time_run_second': 28.20881152153015}\n"
     ]
    }
   ],
   "source": [
    "db_connect = DatabaseConnect(uri = config.DWH_SQLALCHEMY_URI)\n",
    "executor = ProjectExecutor(\n",
    "    environment=config.ENVIRONMENT,\n",
    "    project_id=config.GDA_PROJECT_ID,\n",
    "    uri=config.ELROND_URI,\n",
    "    database_name=config.ELROND_DATABASE,\n",
    "    db = db_connect\n",
    ")\n",
    "pprint(executor.clean().__dict__)\n",
    "pprint(executor.backup_docs().__dict__)\n",
    "pprint(executor.backup_trans().__dict__)\n",
    "pprint(executor.backup_performance().__dict__)\n",
    "pprint(executor.backup_field().__dict__)\n",
    "pprint(executor.dim_field().__dict__)\n",
    "pprint(executor.fact_document().__dict__)\n",
    "pprint(executor.fact_performance().__dict__)\n",
    "pprint(executor.fact_data_extract().__dict__)\n",
    "\n",
    "db_connect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "import pendulum\n",
    "\n",
    "import config\n",
    "import function as func\n",
    "\n",
    "from schema.fact_document import FactDocumentModel\n",
    "from schema.fact_performance import FactPerformanceModel\n",
    "from db_connect import EngineConnect as DatabaseConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MvlStpOcrExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *kwargs,\n",
    "        environment: str,\n",
    "        uri: str,\n",
    "        database_name: str,\n",
    "        docs_collection_name: str, \n",
    "        trans_collection_name: str,\n",
    "        performance_collection_name: str,\n",
    "        db: DatabaseConnect\n",
    "    ):\n",
    "        self.environment = environment\n",
    "        self.uri = uri\n",
    "        self.database_name = database_name\n",
    "        self.docs_collection_name = docs_collection_name\n",
    "        self.trans_collection_name = trans_collection_name\n",
    "        self.performance_collection_name = performance_collection_name\n",
    "        self.db = db\n",
    "        self.start_run = time.time()\n",
    "        self.maxSevSelDelay = 20000\n",
    "        self.start = config.start\n",
    "        self.query = config.ECLAIMS_QUERY\n",
    "        self.performance_query = config.MVL_STP_OCR_PERFORMANCE_QUERY\n",
    "        self.project_id = '5db5c87345052400142992e9'\n",
    "        self.project_name = '148_191004_124_MVL_STP_OCR'\n",
    "        self.backup_dir = \"/usr/local/airflow/storage/\"\n",
    "        self.project_backup_dir = '148_191004_124_MVL_STP_OCR/'\n",
    "        self.project_docs_dir = 'docs/'\n",
    "        self.project_trans_dir = 'trans/'\n",
    "        self.project_performance_dir = 'performance/'\n",
    "        self.backup_file_type = '.pickle'\n",
    "        self.schema = config.DWH_ANALYTIC_SCHEMA\n",
    "        self.fact_document_table = 'fact_document'\n",
    "        self.fact_performancec_table = 'fact_performance'\n",
    "        \n",
    "    def get_docs_and_trans(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_docs = pickle.load(open('./backup/docs/' + self.project_id + '.pickle', 'rb'))\n",
    "            obj_trans = pickle.load(open('./backup/trans/' + self.project_id + '.pickle', 'rb'))\n",
    "        else: \n",
    "            obj_docs = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_docs_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "            obj_trans = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_trans_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_docs = [item for item in obj_docs]\n",
    "        data_trans = [item for item in obj_trans]\n",
    "        return data_docs, data_trans\n",
    "\n",
    "    def get_performance(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_performance = pickle.load(open('./backup/performance/' + self.project_id + '.pickle', 'rb'))\n",
    "        else:\n",
    "            obj_performance = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_performance_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_performance = [item for item in obj_performance]\n",
    "        return data_performance\n",
    "        \n",
    "    def fact_document(self):\n",
    "        datas = []\n",
    "        data_docs, data_trans = self.get_docs_and_trans()\n",
    "        list_created = [{'doc_id': func.bson_object_to_string(data['_id']), 'created_date': data['created_date']} for data in data_docs]\n",
    "        for data in data_trans:\n",
    "            if len(data['records']) == 0:\n",
    "                continue\n",
    "            created_date = func.created_date_of_docs_by_id(func.bson_object_to_string(data['doc_id']), list_created)\n",
    "            if created_date == None:\n",
    "                (import_date_key_utc_7, import_time_key_utc_7, created_date_utc_7) = (None, None, None)\n",
    "            else:\n",
    "                created_date_utc_7 = created_date + datetime.timedelta(hours = 7)\n",
    "                import_date_key_utc_7, import_time_key_utc_7 = func.handle_date_to_date_and_time_id(created_date_utc_7)\n",
    "            last_modified_utc_7 = data['last_modified'] + datetime.timedelta(hours = 7)\n",
    "            export_date_key_utc_7, export_time_key_utc_7 = func.handle_date_to_date_and_time_id(last_modified_utc_7)\n",
    "            _obj = FactDocumentModel(\n",
    "                project_id = self.project_id,\n",
    "                document_id = func.bson_object_to_string(data['doc_id']),\n",
    "                doc_set_id =  func.bson_object_to_string(data['doc_set_id']),\n",
    "                remark_code = None,\n",
    "                remark_description = data['records'][0]['REMARK'],\n",
    "                import_date_key = import_date_key_utc_7,\n",
    "                import_time_key = import_time_key_utc_7,\n",
    "                export_date_key = export_date_key_utc_7,\n",
    "                export_time_key = export_time_key_utc_7,\n",
    "                import_timestamp = created_date_utc_7,\n",
    "                export_timestamp = last_modified_utc_7,\n",
    "            )\n",
    "            datas.append(_obj)\n",
    "        if datas != []:\n",
    "            print(datas[0].__dict__)\n",
    "        self.db.create([item.__dict__ for item in datas], self.schema, self.fact_document_table)\n",
    "    \n",
    "        \n",
    "    def fact_performance(self):\n",
    "        datas = []\n",
    "        data_performance = self.get_performance()\n",
    "        for performance in data_performance:\n",
    "            captured_date_timestamp = datetime.datetime.strptime(performance['captured_date'], '%d/%m/%Y')\n",
    "            obj_ = FactPerformanceModel(\n",
    "                    ori_id = func.bson_object_to_string(performance['_id']),  \n",
    "                    project_id = self.project_id,  \n",
    "                    group_id = performance['group_id'],  \n",
    "                    document_id = performance['documentId'],  \n",
    "                    reworked = performance['has_rework'],  \n",
    "                    work_type_id = func.get_working_type_id_by_name(performance['work_type']),  \n",
    "                    process_key = func.get_process_id_performance(performance['type']),  \n",
    "                    number_of_record = performance['records'],  \n",
    "                    user_name = performance['username'], \n",
    "                    ip = None, \n",
    "                    captured_date_timestamp = captured_date_timestamp,  \n",
    "                    captured_date_key = func.time_to_date_key(captured_date_timestamp),  \n",
    "                    captured_time_key = 0,  \n",
    "                    total_time_second = performance['total_time']/100     \n",
    "            )\n",
    "            datas.append(obj_)\n",
    "        if datas != []:\n",
    "            print(datas[0].__dict__)\n",
    "        self.db.create([item.__dict__ for item in datas], self.schema, self.fact_performancec_table)\n",
    "    \n",
    "    def check_connect(self):\n",
    "        if self.environment == 'development':\n",
    "            (status, content, time_run) = (True, \"good!\",  time.time()- self.start_run)\n",
    "        else:\n",
    "            client = MongoClient(self.uri, serverSelectionTimeoutMS= self.maxSevSelDelay)\n",
    "            client.server_info()\n",
    "            client.close()\n",
    "            (status, content, time_run) = (True, \"good!\",  time.time()-self.start_run)\n",
    "        print('check_connect done!')\n",
    "        return {\"status\": status, \"content\": content, \"time\": time_run}\n",
    "    \n",
    "    def backup_performance(self):\n",
    "        if self.environment == 'development':\n",
    "            objects = pickle.load(open('./backup/performance/' + self.project_id + '.pickle', 'rb'))\n",
    "            data_objects = [item for item in objects]\n",
    "            handle = open('./backup_test/performance_' + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "            pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "        else:\n",
    "            client = MongoClient(self.uri)\n",
    "            data_query = client[self.database_name][self.performance_collection_name].find(self.query)\n",
    "            data_objects = [item for item in data_query]\n",
    "            client.close()\n",
    "            handle = open(self.backup_dir + self.project_backup_dir + self.project_performance_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "            pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "        print('backup_performance done!')\n",
    "        \n",
    "    def backup_docs(self):\n",
    "        if self.environment == 'development':\n",
    "            objects = pickle.load(open('./backup/docs/' + self.project_id + '.pickle', 'rb'))\n",
    "            data_objects = [item for item in objects]\n",
    "            handle = open('./backup_test/docs_' + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "            pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "        else:\n",
    "            client = MongoClient(self.uri)\n",
    "            data_query = client[self.database_name][self.docs_collection_name].find(self.query)\n",
    "            data_objects = [item for item in data_query]\n",
    "            client.close()\n",
    "            handle = open(self.backup_dir + self.project_backup_dir + self.project_docs_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "            pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "        print('backup_docs done!')\n",
    "    \n",
    "    def backup_trans(self):\n",
    "        if self.environment == 'development':\n",
    "            objects = pickle.load(open('./backup/trans/' + self.project_id + '.pickle', 'rb'))\n",
    "            data_objects = [item for item in objects]\n",
    "            handle = open('./backup_test/tran_' + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "            pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "        else:\n",
    "            client = MongoClient(self.uri)\n",
    "            data_query = client[self.database_name][self.trans_collection_name].find(self.query)\n",
    "            data_objects = [item for item in data_query]\n",
    "            client.close()\n",
    "            handle = open(self.backup_dir + self.project_backup_dir + self.project_trans_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type , 'wb')\n",
    "            pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            handle.close()\n",
    "        print('backup_trans done!')\n",
    "    \n",
    "    def report(self):\n",
    "        print('report done!')\n",
    "    \n",
    "    def clean(self): \n",
    "        if self.environment == 'development' or self.environment == 'production':\n",
    "            now = self.start - timedelta(days=1)\n",
    "            file_name = str(now.strftime(\"%Y-%m-%d\"))\n",
    "            docs_file_path = self.backup_dir + self.project_backup_dir + self.project_docs_dir + file_name + self.backup_file_type\n",
    "            trans_file_path = self.backup_dir + self.project_backup_dir + self.project_trans_dir + file_name + self.backup_file_type\n",
    "            performance_file_path = self.backup_dir + self.project_backup_dir + self.project_performance_dir + file_name + self.backup_file_type\n",
    "            if os.path.exists(performance_file_path):\n",
    "                os.remove(performance_file_path)\n",
    "            else:\n",
    "                print(\"The performance_file_path does not exist\")\n",
    "            if os.path.exists(docs_file_path):\n",
    "                os.remove(docs_file_path)\n",
    "            else:\n",
    "                print(\"The docs_file_path does not exist\")\n",
    "            if os.path.exists(trans_file_path):\n",
    "                os.remove(trans_file_path)\n",
    "            else:\n",
    "                print(\"The trans_file_path does not exist\")\n",
    "        print('clean done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance_file_path does not exist\n",
      "The docs_file_path does not exist\n",
      "The trans_file_path does not exist\n",
      "clean done!\n",
      "backup_docs done!\n",
      "backup_trans done!\n",
      "backup_performance done!\n",
      "{'project_id': '5db5c87345052400142992e9', 'document_id': '5ffa64ed474eb70010c3cc8b', 'doc_set_id': '5ffa64ed474eb70010c3cc8a', 'import_time_key': 92237, 'import_date_key': 20210110, 'export_time_key': 92405, 'export_date_key': 20210110, 'import_timestamp': datetime.datetime(2021, 1, 10, 9, 22, 37, 828000), 'export_timestamp': datetime.datetime(2021, 1, 10, 9, 24, 5, 906000), 'remark_code': None, 'remark_description': ''}\n",
      "{'ori_id': '5dfccb6652439f0014026014', 'project_id': '5db5c87345052400142992e9', 'group_id': None, 'document_id': '5dfc2ef1691f5300101137cc', 'reworked': False, 'work_type_id': 1, 'process_key': 3, 'number_of_record': 2, 'user_name': 'hiennm', 'ip': None, 'captured_date_timestamp': datetime.datetime(2019, 12, 20, 0, 0), 'captured_date_key': 20191220, 'captured_time_key': 0, 'total_time_second': 346.93}\n",
      "report done!\n"
     ]
    }
   ],
   "source": [
    "db_connect = DatabaseConnect(uri = config.DWH_SQLALCHEMY_URI)\n",
    "executor = MvlStpOcrExecutor(\n",
    "    environment=config.ENVIRONMENT,\n",
    "    uri=config.ELROND_URI,\n",
    "    database_name=config.ELROND_DATABASE,\n",
    "    docs_collection_name= config.MVL_STP_OCR_DOCS_COLLECTION, \n",
    "    trans_collection_name= config.MVL_STP_OCR_TRANS_COLLECTION,\n",
    "    performance_collection_name = config.MVL_STP_OCR_PERFORMANCE_COLLECTION,\n",
    "    db = db_connect\n",
    ")\n",
    "executor.clean()\n",
    "executor.backup_docs()\n",
    "executor.backup_trans()\n",
    "executor.backup_performance()\n",
    "executor.fact_document()\n",
    "executor.fact_performance()\n",
    "executor.report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_params = {\n",
    "    'dag_id': \"dwh_mvl_stp_ocr_project_daily_tmp\",\n",
    "    'start_date': datetime.datetime(2021, 1, 6, tzinfo=config.LOCAL_TIME_ZONE),\n",
    "    'schedule_interval': '20 5 * * *'\n",
    "}\n",
    "\n",
    "dag = DAG(**dag_params)\n",
    "\n",
    "clean = PythonOperator(task_id='clean', python_callable=executor.clean, dag=dag)\n",
    "check_connect = PythonOperator(task_id='check_connect', python_callable=executor.check_connect, dag=dag)\n",
    "backup_docs_json = PythonOperator(task_id='backup_docs_json', python_callable=executor.backup_docs_json, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "backup_trans_json = PythonOperator(task_id='backup_trans_json', python_callable=executor.backup_trans_json, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "backup_performance = PythonOperator(task_id='backup_performance', python_callable=executor.backup_performance, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "\n",
    "fact_performance = PythonOperator(task_id='fact_performance', python_callable=executor.fact_performance, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "fact_document = PythonOperator(task_id='fact_document', python_callable=executor.fact_document, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "\n",
    "report = PythonOperator(task_id='report', python_callable=executor.report, dag=dag, trigger_rule=TriggerRule.ALL_DONE)\n",
    "\n",
    "clean >> check_connect >> [backup_trans_json, backup_docs_json, backup_performance]\n",
    "\n",
    "fact_performance.set_upstream(backup_performance)\n",
    "fact_document.set_upstream([backup_trans_json, backup_docs_json])\n",
    "\n",
    "[fact_performance, fact_document] >> report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pickle.load(open('./backup/trans/' + '5db5c87345052400142992e9' + '.pickle', 'rb'))\n",
    "docs = pickle.load(open('./backup/docs/' + '5db5c87345052400142992e9' + '.pickle', 'rb'))\n",
    "performance = pickle.load(open('./backup/performance/' + '5db5c87345052400142992e9' + '.pickle', 'rb'))\n",
    "# keyed_data\n",
    "# qc\n",
    "\n",
    "# pprint(performance)\n",
    "x = []\n",
    "for data in docs:\n",
    "    pprint(data)\n",
    "    break\n",
    "    ocr_results = data['records'][0]['system_data'][0]['ocr_data'][0]['ocr_results']\n",
    "    for ocr_result in ocr_results:\n",
    "        field_name = ocr_result['field_name']\n",
    "        if field_name not in x: \n",
    "            x.append(field_name)\n",
    "print(x)\n",
    "['address', 'birthday', 'expiry', 'home_town', 'id', 'issue_at', 'issue_date', 'name', 'sex']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

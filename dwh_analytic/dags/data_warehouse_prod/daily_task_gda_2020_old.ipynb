{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from pymongo import MongoClient\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "import pendulum\n",
    "import config\n",
    "\n",
    "import function as func\n",
    "\n",
    "from schema.fact_document import FactDocumentModel\n",
    "from schema.fact_performance import FactPerformanceModel\n",
    "from schema.fact_data_extraction import FactDataExtractionModel\n",
    "from db_connect import EngineConnect as DatabaseConnect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadingData:\n",
    "    def get_docs_and_trans(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_docs = pickle.load(open('./backup/' + self.project_backup_dir + self.project_docs_dir + self.project_id + '.pickle', 'rb'))\n",
    "            obj_trans = pickle.load(open('./backup/' + self.project_backup_dir + self.project_trans_dir + self.project_id + '.pickle', 'rb'))\n",
    "        else: \n",
    "            obj_docs = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_docs_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "            obj_trans = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_trans_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_docs = [item for item in obj_docs]\n",
    "        data_trans = [item for item in obj_trans]\n",
    "        return data_docs, data_trans\n",
    "    \n",
    "    def get_performance(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_performance = pickle.load(open('./backup/' +self.project_backup_dir + self.project_performance_dir + self.project_id + '.pickle', 'rb'))\n",
    "        else:\n",
    "            obj_performance = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_performance_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_performance = [item for item in obj_performance]\n",
    "        return data_performance\n",
    "    \n",
    "    def get_field(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_load = pickle.load(open('./backup/' +self.project_backup_dir + self.project_field_dir + self.project_id + '.pickle', 'rb'))\n",
    "        else:\n",
    "            obj_load = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_field_dir + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        obj_list = [item for item in obj_load]\n",
    "        return obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_set_id': ('5ff2df89474eb70010c1a4de',),\n",
      " 'document_id': '5ff2df89474eb70010c1a5aa',\n",
      " 'document_key': 1,\n",
      " 'export_date_key': 20210111,\n",
      " 'export_time_key': 85508,\n",
      " 'export_timestamp': datetime.datetime(2021, 1, 11, 8, 55, 8, 149000),\n",
      " 'import_date_key': 20210104,\n",
      " 'import_time_key': 162737,\n",
      " 'import_timestamp': datetime.datetime(2021, 1, 4, 16, 27, 37, 617000),\n",
      " 'ori_document_id': '5ffbaffc489b01001ed5a29f',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'remark_code': None,\n",
      " 'remark_description': None}\n",
      "{'captured_date_key': 2021111,\n",
      " 'captured_date_timestamp': datetime.datetime(2021, 1, 11, 7, 2, 36, 9000),\n",
      " 'captured_time_key': 7236,\n",
      " 'document_id': '5ff2df8b474eb70010c1ad5a',\n",
      " 'document_key': 1678,\n",
      " 'group_id': None,\n",
      " 'ip': '10.1.29.163',\n",
      " 'number_of_character': 429,\n",
      " 'number_of_field': 30,\n",
      " 'number_of_item': 5,\n",
      " 'number_of_record': 5,\n",
      " 'ori_performance_id': '5ffb959caa3d950371f96ef3',\n",
      " 'performance_key': 1,\n",
      " 'process_key': 3,\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'reworked': False,\n",
      " 'total_time_second': 225386,\n",
      " 'user_name': 'thunna',\n",
      " 'work_type_key': 1}\n",
      "{'doc_set_id': '5ff2df88474eb70010c1a0e7',\n",
      " 'document_id': '5ff2df88474eb70010c1a1a5',\n",
      " 'document_key': 2054,\n",
      " 'field_name': 'lfdNr',\n",
      " 'field_value': '121',\n",
      " 'last_modified_date_key': 2021107,\n",
      " 'last_modified_time_key': 112525,\n",
      " 'last_modified_timestamp': datetime.datetime(2021, 1, 7, 11, 25, 25, 218000),\n",
      " 'ori_document_id': '5ff2df88474eb70010c1a1a5',\n",
      " 'performance_key': None,\n",
      " 'process_key': 3,\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'user_name': 'hangntt_2'}\n"
     ]
    }
   ],
   "source": [
    "class GdaExecutor(LoadingData, BackupData):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *kwargs,\n",
    "        environment: str,\n",
    "        uri: str,\n",
    "        database_name: str,\n",
    "        docs_collection_name: str, \n",
    "        trans_collection_name: str,\n",
    "        performance_collection_name: str,\n",
    "        db: DatabaseConnect\n",
    "    ):\n",
    "        self.environment = environment\n",
    "        self.uri = uri\n",
    "        self.database_name = database_name\n",
    "        self.docs_collection_name = docs_collection_name\n",
    "        self.trans_collection_name = trans_collection_name\n",
    "        self.performance_collection_name = performance_collection_name\n",
    "        self.db = db\n",
    "        self.maxSevSelDelay = 20000\n",
    "        self.start = config.start\n",
    "        self.query = config.GDA_QUERY\n",
    "        self.performance_query = config.GDA_PERFORMANCE_QUERY\n",
    "        self.project_id = config.GDA_PROJECT_ID\n",
    "        self.project_name = config.GDA_PROJECT_NAME\n",
    "        self.backup_dir = config.BACKUP_DIR\n",
    "        self.project_backup_dir = config.GDA_BACKUP_DIR\n",
    "        self.project_docs_dir = config.GDA_DOCS_DIR\n",
    "        self.project_trans_dir = config.GDA_TRANS_DIR\n",
    "        self.project_field_dir = config.GDA_FIELD_DIR\n",
    "        self.project_performance_dir = config.GDA_PERFORMANCE_DIR\n",
    "        self.backup_file_type = config.BACKUP_FILE_TYPE\n",
    "        self.schema = config.DWH_ANALYTIC_SCHEMA\n",
    "        self.fact_document_table = config.DWH_FACT_DOCUMENT_TABLE\n",
    "        self.fact_performance_table = config.DWH_FACT_PERFORMANCE_TABLE\n",
    "        self.fact_data_extraction_table = config.DWH_FACT_DATA_EXTRACTION_TABLE\n",
    "        self.fact_report_table = config.DWH_FACT_REPORT_ETL_TABLE\n",
    "        self.schedule_type = 'daily'\n",
    "        self.schedule_date_key = 20200113\n",
    "        self.schedule_time_key = 50000\n",
    "        self.reports = []\n",
    "        self.document_key_checks = []\n",
    "        self.performance_key_checks = []\n",
    "    \n",
    "    def clean(self):\n",
    "        report = initial_report('clean', self.project_id, self.schedule_type, self.schedule_date_key, self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development' or self.environment == 'production':\n",
    "                now = self.start - timedelta(days=1)\n",
    "                file_name = str(now.strftime(\"%Y-%m-%d\"))\n",
    "                list_path = [\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_docs_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_trans_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_performance_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_field_dir + file_name + self.backup_file_type,\n",
    "                ]\n",
    "                for item in list_path:\n",
    "                    if os.path.exists(item):\n",
    "                        os.remove(item)\n",
    "                    else:\n",
    "                        report.description = \"The {path} does not exist\".format(path = item)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "   \n",
    "    def fact_document(self):\n",
    "        report = initial_report('fact_document', self.project_id, self.schedule_type, self.schedule_date_key, self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            datas = []\n",
    "            data_docs, data_trans = self.get_docs_and_trans()\n",
    "            list_created = [{'doc_id': func.bson_object_to_string(data['_id']), 'created_date': data['created_date']} for data in data_docs]\n",
    "            _id = self.db.get_max_id_table(schema = self.schema, table = self.fact_document_table, col = 'document_key')\n",
    "            if _id == None:\n",
    "                _id = 1\n",
    "            else:\n",
    "                _id+=1\n",
    "            for data in data_trans:\n",
    "                if len(data['records']) == 0:\n",
    "                    continue\n",
    "                created_date_utc_7  = func.created_date_of_docs_by_id(func.bson_object_to_string(data['doc_id']), list_created) + datetime.timedelta(hours = 7)\n",
    "                last_modified_utc_7 = data['last_modified'] + datetime.timedelta(hours = 7)\n",
    "                import_date_key_utc_7, import_time_key_utc_7 = func.handle_date_to_date_and_time_id(created_date_utc_7)\n",
    "                export_date_key_utc_7, export_time_key_utc_7 = func.handle_date_to_date_and_time_id(last_modified_utc_7)\n",
    "                document_id = func.bson_object_to_string(data['doc_id'])\n",
    "                doc_set_id = func.bson_object_to_string(data['doc_set_id']),\n",
    "                _obj = FactDocumentModel(\n",
    "                    document_key = _id,\n",
    "                    ori_document_id = func.bson_object_to_string(data['_id']),\n",
    "                    project_id = self.project_id,\n",
    "                    document_id = document_id,\n",
    "                    doc_set_id =  doc_set_id,\n",
    "                    remark_code = None,\n",
    "                    remark_description = None,\n",
    "                    import_date_key = import_date_key_utc_7,\n",
    "                    import_time_key = import_time_key_utc_7,\n",
    "                    export_date_key = export_date_key_utc_7,\n",
    "                    export_time_key = export_time_key_utc_7,\n",
    "                    import_timestamp = created_date_utc_7,\n",
    "                    export_timestamp = last_modified_utc_7,\n",
    "                )\n",
    "                self.document_key_checks.append({'document_key': _id, 'document_id': document_id, 'doc_set_id': doc_set_id})\n",
    "                datas.append(_obj)\n",
    "                _id+=1\n",
    "            if datas != []:\n",
    "                pprint(datas[0].__dict__)\n",
    "            self.db.create([item.__dict__ for item in datas], self.schema, self.fact_document_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "\n",
    "    def get_document_key_by_document_id(self, document_id: str):\n",
    "        document_key = None\n",
    "        for item in self.document_key_checks:\n",
    "            if item['document_id'] == document_id:\n",
    "                document_key = item['document_key']\n",
    "                break\n",
    "        return document_key\n",
    "    \n",
    "  \n",
    "    def fact_performance(self):\n",
    "        report = initial_report('fact_performance', self.project_id, self.schedule_type, self.schedule_date_key, self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            datas = []\n",
    "            data_performance = self.get_performance()\n",
    "            _id = self.db.get_max_id_table(schema = self.schema, table = self.fact_performance_table, col ='performance_key')\n",
    "            if _id == None:\n",
    "                _id = 1\n",
    "            else:\n",
    "                _id+=1\n",
    "            for performance in data_performance:\n",
    "                captured_date_timestamp_utc_7 = performance['time'] + datetime.timedelta(hours = 7)\n",
    "                document_key = self.get_document_key_by_document_id(performance['doc_id'])\n",
    "                obj_ = FactPerformanceModel(\n",
    "                        performance_key = _id,\n",
    "                        ori_performance_id = func.bson_object_to_string(performance['_id']),\n",
    "                        document_key = document_key,\n",
    "                        project_id = self.project_id,  \n",
    "                        group_id = performance['group_id'],  \n",
    "                        document_id = performance['doc_id'],  \n",
    "                        reworked = func.int_to_bool(performance['rework_count']),  \n",
    "                        work_type_key = func.get_working_type_id_by_name(performance['work_type']),  \n",
    "                        process_key = func.get_process_key_performance_gda(performance['type'], performance['task_def_key']),  \n",
    "                        number_of_record = performance['records'],\n",
    "                        number_of_item = performance['items'],  \n",
    "                        number_of_field = performance['fields'],\n",
    "                        number_of_character = performance['chars'],  \n",
    "                        user_name = performance['username'], \n",
    "                        ip = performance['ip'], \n",
    "                        captured_date_timestamp = captured_date_timestamp_utc_7,  \n",
    "                        captured_date_key = func.time_to_date_key(captured_date_timestamp_utc_7),  \n",
    "                        captured_time_key = func.time_to_time_key(captured_date_timestamp_utc_7),  \n",
    "                        total_time_second = performance['total_time']    \n",
    "                )\n",
    "                datas.append(obj_)\n",
    "                self.performance_key_checks.append({'performance_key': _id, 'user_name': performance['username'], \n",
    "                                                    'document_key': document_key, 'module_type': performance['type'], \n",
    "                                                    'task_def_key': performance['task_def_key']})\n",
    "                _id+=1\n",
    "            if datas != []:\n",
    "                pprint(datas[0].__dict__)\n",
    "            self.db.create([item.__dict__ for item in datas], self.schema, self.fact_performance_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "            pprint(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "    \n",
    "    def get_performance_key(self, document_key: str, user_name: str, module_type: str, task_def_key: str):\n",
    "        performance_key = None\n",
    "        for item in self.performance_key_checks:\n",
    "            if item['user_name'] == user_name and item['module_type'] == module_type and item['document_key'] == document_key and item['task_def_key'] == task_def_key:\n",
    "                performance_key = item['performance_key']\n",
    "                break\n",
    "        return performance_key\n",
    "                                                   \n",
    "    def fact_data_extract(self):\n",
    "        report = initial_report('fact_data_extraction', self.project_id, self.schedule_type, self.schedule_date_key, self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            data_docs, data_trans = self.get_docs_and_trans()\n",
    "            col_ignores = ['ImagePath']\n",
    "            results = []\n",
    "            for data in data_docs:\n",
    "                records = data['records']\n",
    "                document_id = func.bson_object_to_string(data['_id'])          \n",
    "                document_key = self.get_document_key_by_document_id(document_id)\n",
    "                doc_set_id = func.bson_object_to_string(data['doc_set_id'])\n",
    "                for record in records:\n",
    "                    for key, value in record.items():\n",
    "                        if key == 'keyed_data':\n",
    "                            for keyed_data in value:\n",
    "                                source = keyed_data['source']\n",
    "                                task_def_key = keyed_data['task_def_key']\n",
    "                                task_id = keyed_data['task_id']\n",
    "                                section = keyed_data['section']\n",
    "                                reason = keyed_data['reason']\n",
    "                                data_needed = keyed_data['data'][0].items()\n",
    "                                last_modified_utc_7 = keyed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                                user_name = keyed_data['keyer']\n",
    "                                performance_key = None\n",
    "                                if source != 'queue_transform' and task_def_key.startswith('Type'):\n",
    "                                    process_key = 3 # human input keyed_data kpi\n",
    "                                    performance_key = self.get_performance_key(document_key, user_name, 'keying', task_def_key) \n",
    "                                if source != 'queue_transform' and task_def_key == 'Verify_Hold_Type':\n",
    "                                    process_key = 12 # human check bad_image keyed_data not kpi                               \n",
    "                                elif source == 'queue_transform' and task_def_key.startswith('Type'):\n",
    "                                    process_key = 4 # 'machine save input keyed_data'\n",
    "                                elif source != 'queue_transform' and task_def_key.startswith('Proof'):\n",
    "                                    process_key = 5 # human qc input keyed_data' kpi\n",
    "                                    performance_key = self.get_performance_key(document_key, user_name, 'keying', task_def_key)                                  \n",
    "                                elif source == 'queue_transform' and task_def_key.startswith('Proof'):\n",
    "                                    process_key = 6 # 'machine save qc keyed_data'\n",
    "                                for field_name, field_value_dict in data_needed:\n",
    "                                    if field_name in col_ignores:\n",
    "                                        continue\n",
    "                                    _obj = FactDataExtractionModel(\n",
    "                                        document_key = document_key,\n",
    "                                        performance_key = performance_key,\n",
    "                                        ori_document_id = document_id,\n",
    "                                        project_id = self.project_id,\n",
    "                                        document_id = document_id,\n",
    "                                        doc_set_id =  doc_set_id,\n",
    "                                        last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                        last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                        last_modified_timestamp = last_modified_utc_7,\n",
    "                                        user_name = user_name,\n",
    "                                        process_key = process_key,\n",
    "                                        field_name = field_name,\n",
    "                                        field_value = field_value_dict['text']\n",
    "                                    )\n",
    "                                    results.append(_obj)\n",
    "\n",
    "                        elif key == 'final_data':\n",
    "                            final_data = value[0]\n",
    "                            data_needed = final_data['data'][0].items()\n",
    "                            last_modified_utc_7 = final_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                            user_name = final_data['keyer']\n",
    "                            process_key = 10\n",
    "                            for field_name, field_value_dict in data_needed:\n",
    "                                if field_name in col_ignores:\n",
    "                                    continue\n",
    "                                _obj = FactDataExtractionModel(\n",
    "                                    document_key = document_key,\n",
    "                                    performance_key = None,\n",
    "                                    ori_document_id = document_id,\n",
    "                                    project_id = self.project_id,\n",
    "                                    document_id = document_id,\n",
    "                                    doc_set_id =  doc_set_id,\n",
    "                                    last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                    last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                    last_modified_timestamp = last_modified_utc_7,\n",
    "                                    user_name = user_name,\n",
    "                                    process_key = process_key,\n",
    "                                    field_name = field_name,\n",
    "                                    field_value = field_value_dict['text']\n",
    "                                )\n",
    "                                results.append(_obj)\n",
    "\n",
    "                        elif key == 'qc_ed_data':\n",
    "                            qc_ed_data = value[0][0]\n",
    "                            if 'qc_fields_err' not in qc_ed_data.keys():\n",
    "                                continue\n",
    "                            qc_ed_data_err = qc_ed_data['qc_fields_err']\n",
    "                            data_needed = qc_ed_data_err[0].items()\n",
    "                            last_modified_utc_7 = qc_ed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                            user_name = qc_ed_data['keyer']\n",
    "                            process_key = 8\n",
    "                            performance_key = None\n",
    "                            performance_key = self.get_performance_key(document_key, user_name, 'qc', task_def_key) \n",
    "                            for field_name, field_value_dict in data_needed:\n",
    "                                if field_name in col_ignores:\n",
    "                                    continue\n",
    "                                _obj = FactDataExtractionModel(\n",
    "                                    document_key = document_key,\n",
    "                                    performance_key = performance_key,\n",
    "                                    ori_document_id = document_id,\n",
    "                                    project_id = self.project_id,\n",
    "                                    document_id = document_id,\n",
    "                                    doc_set_id =  doc_set_id,\n",
    "                                    last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                    last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                    last_modified_timestamp = last_modified_utc_7,\n",
    "                                    user_name = user_name,\n",
    "                                    process_key = process_key,\n",
    "                                    field_name = field_name,\n",
    "                                    field_value = field_value_dict['text']\n",
    "                                )\n",
    "                                results.append(_obj)\n",
    "\n",
    "                        elif key == 'apr_ed_data':\n",
    "                            print('here')\n",
    "            if len(results) != 0:\n",
    "                pprint(results[0].__dict__)\n",
    "            self.db.create([item.__dict__ for item in results], self.schema, self.fact_data_extraction_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "            pprint(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "\n",
    "    def report_upload(self):\n",
    "        self.db.create([item.__dict__ for item in self.reports], self.schema, self.fact_report_table)\n",
    "    \n",
    "db_connect = DatabaseConnect(uri = config.DWH_SQLALCHEMY_URI)\n",
    "executor = GdaExecutor(\n",
    "    environment=config.ENVIRONMENT,\n",
    "    uri=config.ELROND_URI,\n",
    "    database_name=config.ELROND_DATABASE,\n",
    "    docs_collection_name= config.GDA_DOCS_COLLECTION, \n",
    "    trans_collection_name= config.GDA_TRANS_COLLECTION,\n",
    "    performance_collection_name = config.GDA_PERFORMANCE_COLLECTION,\n",
    "    db = db_connect\n",
    ")\n",
    "executor.clean()\n",
    "executor.fact_document()\n",
    "executor.fact_performance()\n",
    "executor.fact_data_extract() \n",
    "executor.report_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#             self.db.create([item.__dict__ for item in results], self.schema, self.fact_data_extraction_table)\n",
    "#                     elif key == 'qc_ed_data':\n",
    "#                         qc_ed_data = value[0][0]\n",
    "#                         if 'qc_fields_err' not in qc_ed_data.keys():\n",
    "#                             pass\n",
    "#                         else:\n",
    "#                             data_needed = qc_ed_data['data'].items()\n",
    "#                             last_modified_utc_7 = qc_ed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "#                     elif key == 'apr_ed_data':\n",
    "#                         pprint('here')\n",
    "#                     elif key == 'final_data':\n",
    "#                         final_data = value[0]\n",
    "#                         last_modified_utc_7 = final_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "#                         user_name = final_data['keyer']\n",
    "#                         process_key = 7 # machine save final_data\n",
    "#                         data_needed = final_data['data'][0].items()\n",
    "#                         for field_name, field_value_dict in data_needed:\n",
    "#                             if field_name in col_ignores:\n",
    "#                                 _obj = FactDataExtractionModel(\n",
    "#                                     project_id = self.project_id,\n",
    "#                                     document_id = func.bson_object_to_string(data['_id']),\n",
    "#                                     doc_set_id =  func.bson_object_to_string(data['doc_set_id']),\n",
    "#                                     last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "#                                     last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "#                                     last_modified_timestamp = last_modified_utc_7,\n",
    "#                                     user_name = user_name,\n",
    "#                                     process_key = process_key,\n",
    "#                                     field_name = field_name,\n",
    "#                                     field_value = field_value_dict['text']\n",
    "#                                 )\n",
    "#                                 results.append(_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_params = {\n",
    "    'dag_id': \"dwh_GDA_project_daily_tmp\",\n",
    "    'start_date': datetime.datetime(2021, 1, 6, tzinfo=config.LOCAL_TIME_ZONE),\n",
    "    'schedule_interval': '20 5 * * *'\n",
    "}\n",
    "\n",
    "dag = DAG(**dag_params)\n",
    "\n",
    "clean = PythonOperator(task_id='clean', python_callable=executor.clean, dag=dag)\n",
    "check_connect = PythonOperator(task_id='check_connect', python_callable=executor.check_connect, dag=dag)\n",
    "backup_docs_json = PythonOperator(task_id='backup_docs_json', python_callable=executor.backup_docs_json, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "backup_trans_json = PythonOperator(task_id='backup_trans_json', python_callable=executor.backup_trans_json, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "backup_performance = PythonOperator(task_id='backup_performance', python_callable=executor.backup_performance, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "\n",
    "fact_performance = PythonOperator(task_id='fact_performance', python_callable=executor.fact_performance, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "fact_document = PythonOperator(task_id='fact_document', python_callable=executor.fact_document, dag=dag, trigger_rule=TriggerRule.ALL_SUCCESS)\n",
    "\n",
    "\n",
    "report = PythonOperator(task_id='report', python_callable=executor.report, dag=dag, trigger_rule=TriggerRule.ALL_DONE)\n",
    "\n",
    "clean >> check_connect >> [backup_trans_json, backup_docs_json, backup_performance]\n",
    "\n",
    "fact_performance.set_upstream(backup_performance)\n",
    "fact_document.set_upstream([backup_trans_json, backup_docs_json])\n",
    "\n",
    "[fact_performance, fact_document] >> report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pickle.load(open('./backup/trans/' + '5db5c87345052400142992e9' + '.pickle', 'rb'))\n",
    "docs = pickle.load(open('./backup/docs/' + '5db5c87345052400142992e9' + '.pickle', 'rb'))\n",
    "performance = pickle.load(open('./backup/performance/' + '5db5c87345052400142992e9' + '.pickle', 'rb'))\n",
    "# keyed_data\n",
    "# qc\n",
    "\n",
    "# pprint(performance)\n",
    "x = []\n",
    "for data in docs:\n",
    "    pprint(data)\n",
    "    break\n",
    "    ocr_results = data['records'][0]['system_data'][0]['ocr_data'][0]['ocr_results']\n",
    "    for ocr_result in ocr_results:\n",
    "        field_name = ocr_result['field_name']\n",
    "        if field_name not in x: \n",
    "            x.append(field_name)\n",
    "print(x)\n",
    "['address', 'birthday', 'expiry', 'home_town', 'id', 'issue_at', 'issue_date', 'name', 'sex']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

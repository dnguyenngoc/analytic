{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('/home/duynguyenngoc/Desktop/project/analytic/dwh_analytic/')\n",
    "os.chdir('D:\\\\github\\\\analytic\\\\dwh_analytic\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dags.data_warehouse_prod.schema.init_data import InitData\n",
    "# from dags.data_warehouse_prod.functions.db_connect import EngineConnect as DatabaseConnect\n",
    "# from dags.data_warehouse_prod.settings import config\n",
    "# db_connect = DatabaseConnect(uri = config.DWH_SQLALCHEMY_URI)\n",
    "# db_connect.close()\n",
    "# InitData().dim_project_variable(db_connect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "from dateutil import tz\n",
    "import pendulum\n",
    "import pickle\n",
    "from pymongo import MongoClient\n",
    "sudo \n",
    "from dags.data_warehouse_prod.settings import config\n",
    "from dags.data_warehouse_prod.functions import function as func\n",
    "from dags.data_warehouse_prod.functions.db_connect import EngineConnect as DatabaseConnect\n",
    "from dags.data_warehouse_prod.functions import report as report_func\n",
    "from dags.data_warehouse_prod.schema.fact_document import FactDocumentModel\n",
    "from dags.data_warehouse_prod.schema.dim_field import DimFieldModel\n",
    "from dags.data_warehouse_prod.schema.fact_performance import FactPerformanceModel\n",
    "from dags.data_warehouse_prod.schema.fact_data_extraction import FactDataExtractionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectExecutor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        *kwargs,\n",
    "        environment: str,\n",
    "        project_id: str,\n",
    "        uri: str,\n",
    "        database_name: str = 'elrond',\n",
    "        db: DatabaseConnect\n",
    "    ):\n",
    "        self.backup_dir = config.BACKUP_DIR\n",
    "        self.schema = config.DWH_ANALYTIC_SCHEMA\n",
    "        self.fact_document_table = config.DWH_FACT_DOCUMENT_TABLE\n",
    "        self.fact_performance_table = config.DWH_FACT_PERFORMANCE_TABLE\n",
    "        self.fact_data_extraction_table = config.DWH_FACT_DATA_EXTRACTION_TABLE\n",
    "        self.fact_report_table = config.DWH_FACT_REPORT_ETL_TABLE\n",
    "        self.dim_project_variable_table = config.DWH_DIM_PROJECT_VARIABLE_TABLE\n",
    "        self.dim_field_table = config.DWH_DIM_FIELD_TABLE\n",
    "        self.maxSevSelDelay = 20000\n",
    "        self.start = config.start\n",
    "        self.database_name  = config.ELROND_DATABASE\n",
    "        \n",
    "        self.db = db\n",
    "        self.environment = environment\n",
    "        self.project_id = project_id\n",
    "        \n",
    "        self.var_load = db.get_one_data_by_field_name_and_value('project_id',project_id,self.schema,self.dim_project_variable_table)\n",
    "        self.project_name = config.GDA_PROJECT_NAME\n",
    "        self.docs_collection_name = self.var_load['docs_collection_name']\n",
    "        self.trans_collection_name = self.var_load['trans_collection_name']\n",
    "        self.performance_collection_name = self.var_load['performance_collection_name']\n",
    "        self.field_collection_name = self.var_load['field_collection_name']\n",
    "        self.docs_query = self.var_load['docs_query']\n",
    "        self.trans_query = self.var_load['trans_query']\n",
    "        self.performance_query = self.var_load['performance_query']\n",
    "        self.field_query = self.var_load['field_query']\n",
    "        self.project_backup_dir = self.var_load['project_backup_dir']\n",
    "        self.project_docs_dir = self.var_load['project_docs_dir']\n",
    "        self.project_trans_dir = self.var_load['project_trans_dir']\n",
    "        self.project_performance_dir = self.var_load['project_performance_dir']\n",
    "        self.project_field_dir = self.var_load['project_field_dir']\n",
    "        self.backup_file_type = self.var_load['backup_file_type']\n",
    "        self.schedule_type = self.var_load['schedule_type']\n",
    "        self.schedule_date_key = self.var_load['schedule_date_key']\n",
    "        self.schedule_time_key = self.var_load['schedule_time_key']\n",
    "       \n",
    "        self.reports = []\n",
    "        self.document_key_checks = []\n",
    "        self.performance_key_checks = []\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                      [LOANDING DOCUMENTS & TRANS]\n",
    "#==================================================================================================================================\n",
    "    def get_docs_and_trans(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_docs = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_docs_dir \\\n",
    "                                        + self.project_id + self.backup_file_type, 'rb'))\n",
    "            obj_trans = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_trans_dir \\\n",
    "                                         + self.project_id + self.backup_file_type, 'rb'))\n",
    "        else: \n",
    "            obj_docs = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_docs_dir \\\n",
    "                                        + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "            obj_trans = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_trans_dir \\\n",
    "                                         + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_docs = [item for item in obj_docs]\n",
    "        data_trans = [item for item in obj_trans]\n",
    "        return data_docs, data_trans\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                            [LOANDING PERFORMANCE]\n",
    "#================================================================================================================================== \n",
    "    def get_performance(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_performance = pickle.load(open('dags/data_warehouse_prod/backup/' +self.project_backup_dir + \\\n",
    "                                               self.project_performance_dir + self.project_id + self.backup_file_type, 'rb'))\n",
    "        else:\n",
    "            obj_performance = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_performance_dir \\\n",
    "                                               + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        data_performance = [item for item in obj_performance]\n",
    "        return data_performance\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                 [LOANDING FIELDS]\n",
    "#==================================================================================================================================     \n",
    "    def get_field(self):\n",
    "        if self.environment == 'development':\n",
    "            obj_load = pickle.load(open('dags/data_warehouse_prod/backup/' +self.project_backup_dir + self.project_field_dir \\\n",
    "                                        + self.project_id + self.backup_file_type, 'rb'))\n",
    "        else:\n",
    "            obj_load = pickle.load(open(self.backup_dir + self.project_backup_dir + self.project_field_dir \\\n",
    "                                        + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'rb'))\n",
    "        obj_list = [item for item in obj_load]\n",
    "        return obj_list\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [CHECK CONNECT]\n",
    "#================================================================================================================================== \n",
    "    def check_connect(self):\n",
    "        report = report_func.initial_report('check_connect', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                pass\n",
    "            else:\n",
    "                client = MongoClient(self.uri, serverSelectionTimeoutMS= self.maxSevSelDelay)\n",
    "                client.server_info()\n",
    "                client.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "\n",
    "#==================================================================================================================================\n",
    "#                                                                                                                      [CLEAN TASK]\n",
    "#================================================================================================================================== \n",
    "    def clean(self):\n",
    "        report = report_func.initial_report('clean', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development' or self.environment == 'production':\n",
    "                now = self.start - timedelta(days=1)\n",
    "                file_name = str(now.strftime(\"%Y-%m-%d\"))\n",
    "                list_path = [\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_docs_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_trans_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_performance_dir + file_name + self.backup_file_type,\n",
    "                    self.backup_dir + self.project_backup_dir + self.project_field_dir + file_name + self.backup_file_type,\n",
    "                ]\n",
    "                description = ''\n",
    "                for item in list_path:\n",
    "                    if os.path.exists(item):\n",
    "                        os.remove(item)\n",
    "                    else:\n",
    "                        description +=  ', ' + \"The {path} does not exist\".format(path = item)\n",
    "                report.description = description\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "\n",
    "#==================================================================================================================================\n",
    "#                                                                                                                 [BACKUP DOCUMENT]\n",
    "#================================================================================================================================== \n",
    "    def backup_docs(self):\n",
    "        report = report_func.initial_report('backup_docs', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_docs_dir \\\n",
    "                                           + self.project_id + self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/docs_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.docs_collection_name].find(self.query_docs)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_docs_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                       [BACKUP DOCUMENT TRANSFORM]\n",
    "#================================================================================================================================== \n",
    "    def backup_trans(self):\n",
    "        report = report_func.initial_report('backup_trans', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_trans_dir \\\n",
    "                                           + self.project_id+self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/trans_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.trans_collection_name].find(self.query_trans)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_trans_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type , 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                              [BACKUP PERFORMANCE]\n",
    "#================================================================================================================================== \n",
    "    def backup_performance(self):\n",
    "        report = report_func.initial_report('backup_performance', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir \\\n",
    "                                           + self.project_performance_dir + self.project_id + self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/performance_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.performance_collection_name].find(self.query_performance)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_performance_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                    [BACKUP FIELD]\n",
    "#================================================================================================================================== \n",
    "    def backup_field(self):\n",
    "        report = report_func.initial_report('backup_field', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            if self.environment == 'development':\n",
    "                objects = pickle.load(open('dags/data_warehouse_prod/backup/' + self.project_backup_dir + self.project_field_dir \\\n",
    "                                           + self.project_id + self.backup_file_type, 'rb'))\n",
    "                data_objects = [item for item in objects]\n",
    "                handle = open('dags/data_warehouse_prod/backup/test_data/field_' + str(self.start.strftime(\"%Y-%m-%d\")) \\\n",
    "                              + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            else:\n",
    "                client = MongoClient(self.uri)\n",
    "                data_query = client[self.database_name][self.performance_collection_name].find(self.query_field)\n",
    "                data_objects = [item for item in data_query]\n",
    "                client.close()\n",
    "                handle = open(self.backup_dir + self.project_backup_dir + self.project_performance_dir \\\n",
    "                              + str(self.start.strftime(\"%Y-%m-%d\")) + self.backup_file_type, 'wb')\n",
    "                pickle.dump(data_objects, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                handle.close()\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [UPLOAD_REPORT]\n",
    "#==================================================================================================================================\n",
    "    def report(self):\n",
    "        report_func.upload_report(self.reports, self.schema, self.fact_report_table, self.db)\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [UPLOAD_REPORT]\n",
    "#==================================================================================================================================\n",
    "    def dim_field(self):\n",
    "        report = report_func.initial_report('dim_field', self.project_id, self.schedule_type, self.schedule_date_key,\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            data_fields = self.get_field()\n",
    "            results = []\n",
    "            for data in data_fields:\n",
    "                _obj = DimFieldModel(\n",
    "                        field_key = func.bson_object_to_string(data['_id']),\n",
    "                        project_id = self.project_id,\n",
    "                        name = data['name'],\n",
    "                        control_type =  data['control_type'],\n",
    "                        default_value = data['default_value'],\n",
    "                        counted_character = data['counted_character'],\n",
    "                        is_sub_field = False,\n",
    "                    )\n",
    "                results.append(_obj)\n",
    "            report.status_code = 'PASSED'\n",
    "            self.db.update([item.__dict__ for item in results], self.schema, self.dim_field_table)\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                   [UPLOAD_REPORT]\n",
    "#==================================================================================================================================\n",
    "    def fact_document(self):\n",
    "        report = report_func.initial_report('fact_document', self.project_id, self.schedule_type, self.schedule_date_key,\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            datas = []\n",
    "            data_docs, data_trans = self.get_docs_and_trans()\n",
    "            list_created = [{'doc_id': func.bson_object_to_string(data['_id']), 'created_date': data['created_date']} \\\n",
    "                            for data in data_docs]\n",
    "            _id = self.db.get_max_id_table(schema = self.schema, table = self.fact_document_table, col = 'document_key')\n",
    "            if _id == None: _id = 1\n",
    "            else: _id+=1\n",
    "            for data in data_trans:\n",
    "                if len(data['records']) == 0:\n",
    "                    continue\n",
    "                created_date_utc_7  = func.created_date_of_docs_by_id(func.bson_object_to_string(data['doc_id']), \\\n",
    "                                                                      list_created) + datetime.timedelta(hours = 7)\n",
    "                last_modified_utc_7 = data['last_modified'] + datetime.timedelta(hours = 7)\n",
    "                import_date_key_utc_7, import_time_key_utc_7 = func.handle_date_to_date_and_time_id(created_date_utc_7)\n",
    "                export_date_key_utc_7, export_time_key_utc_7 = func.handle_date_to_date_and_time_id(last_modified_utc_7)\n",
    "                document_id = func.bson_object_to_string(data['doc_id'])\n",
    "                doc_set_id = func.bson_object_to_string(data['doc_set_id']),\n",
    "                _obj = FactDocumentModel(\n",
    "                    document_key = _id,\n",
    "                    ori_document_id = func.bson_object_to_string(data['_id']),\n",
    "                    project_id = self.project_id,\n",
    "                    document_id = document_id,\n",
    "                    doc_set_id =  doc_set_id,\n",
    "                    remark_code = None,\n",
    "                    remark_description = None,\n",
    "                    import_date_key = import_date_key_utc_7,\n",
    "                    import_time_key = import_time_key_utc_7,\n",
    "                    export_date_key = export_date_key_utc_7,\n",
    "                    export_time_key = export_time_key_utc_7,\n",
    "                    import_timestamp = created_date_utc_7,\n",
    "                    export_timestamp = last_modified_utc_7,\n",
    "                )\n",
    "                self.document_key_checks.append({'document_key': _id, 'document_id': document_id, 'doc_set_id': doc_set_id})\n",
    "                datas.append(_obj)\n",
    "                _id+=1\n",
    "            self.db.create([item.__dict__ for item in datas], self.schema, self.fact_document_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "    def get_document_key_by_document_id(self, document_id: str):\n",
    "        document_key = None\n",
    "        for item in self.document_key_checks:\n",
    "            if item['document_id'] == document_id:\n",
    "                document_key = item['document_key']\n",
    "                break\n",
    "        return document_key\n",
    "        \n",
    "#==================================================================================================================================\n",
    "#                                                                                                                [FACT PERFORMANCE]\n",
    "#==================================================================================================================================\n",
    "    def fact_performance(self):\n",
    "        report = report_func.initial_report('fact_performance', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                            self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            datas = []\n",
    "            data_performance = self.get_performance()\n",
    "            _id = self.db.get_max_id_table(schema = self.schema, table = self.fact_performance_table, col ='performance_key')\n",
    "            if _id == None: _id = 1\n",
    "            else: _id+=1\n",
    "            for performance in data_performance:\n",
    "                captured_date_timestamp_utc_7 = performance['time'] + datetime.timedelta(hours = 7)\n",
    "                document_key = self.get_document_key_by_document_id(performance['doc_id'])\n",
    "                obj_ = FactPerformanceModel(\n",
    "                        performance_key = _id,\n",
    "                        ori_performance_id = func.bson_object_to_string(performance['_id']),\n",
    "                        document_key = document_key,\n",
    "                        project_id = self.project_id,  \n",
    "                        group_id = performance['group_id'],  \n",
    "                        document_id = performance['doc_id'],  \n",
    "                        reworked = func.int_to_bool(performance['rework_count']),  \n",
    "                        work_type_key = func.get_working_type_id_by_name(performance['work_type']),  \n",
    "                        process_key = func.get_process_key_performance_gda(performance['type'], performance['task_def_key']),  \n",
    "                        number_of_record = performance['records'],\n",
    "                        number_of_item = performance['items'],  \n",
    "                        number_of_field = performance['fields'],\n",
    "                        number_of_character = performance['chars'],  \n",
    "                        user_name = performance['username'], \n",
    "                        ip = performance['ip'], \n",
    "                        captured_date_timestamp = captured_date_timestamp_utc_7,  \n",
    "                        captured_date_key = func.time_to_date_key(captured_date_timestamp_utc_7),  \n",
    "                        captured_time_key = func.time_to_time_key(captured_date_timestamp_utc_7),  \n",
    "                        total_time_second = performance['total_time']    \n",
    "                )\n",
    "                datas.append(obj_)\n",
    "                self.performance_key_checks.append({'performance_key': _id, 'user_name': performance['username'], \n",
    "                                                    'document_key': document_key, 'module_type': performance['type'], \n",
    "                                                    'task_def_key': performance['task_def_key']})\n",
    "                _id+=1\n",
    "            self.db.create([item.__dict__ for item in datas], self.schema, self.fact_performance_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report\n",
    "        \n",
    "    def get_performance_key(self, document_key: str, user_name: str, module_type: str, task_def_key: str):\n",
    "        performance_key = None\n",
    "        for item in self.performance_key_checks:\n",
    "            if item['user_name'] == user_name and item['module_type'] == module_type \\\n",
    "            and item['document_key'] == document_key and item['task_def_key'] == task_def_key:\n",
    "                performance_key = item['performance_key']\n",
    "                break\n",
    "        return performance_key\n",
    "    \n",
    "#==================================================================================================================================\n",
    "#                                                                                                            [FACT DATA EXTRACTION]\n",
    "#==================================================================================================================================\n",
    "    def fact_data_extract(self):\n",
    "        report = report_func.initial_report('fact_data_extraction', self.project_id, self.schedule_type, self.schedule_date_key, \\\n",
    "                                self.schedule_time_key)\n",
    "        start_run = time.time()\n",
    "        try:\n",
    "            data_docs, data_trans = self.get_docs_and_trans()\n",
    "            col_ignores = ['ImagePath']\n",
    "            results = []                \n",
    "            for data in data_docs:\n",
    "                records = data['records']\n",
    "                document_id = func.bson_object_to_string(data['_id'])          \n",
    "                document_key = self.get_document_key_by_document_id(document_id)\n",
    "                doc_set_id = func.bson_object_to_string(data['doc_set_id'])\n",
    "                for i in range(len(records)):\n",
    "                    record_id = i+1\n",
    "                    record = records[i]\n",
    "                    for key, value in record.items():\n",
    "                        if key == 'keyed_data':\n",
    "                            for keyed_data in value:\n",
    "                                source = keyed_data['source']\n",
    "                                task_def_key = keyed_data['task_def_key']\n",
    "                                data_needed = keyed_data['data'][0].items()\n",
    "                                last_modified_utc_7 = keyed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                                user_name = keyed_data['keyer']\n",
    "                                performance_key = None\n",
    "                                if source != 'queue_transform' and task_def_key.startswith('Type'):\n",
    "                                    process_key = 3 # human input keyed_data kpi\n",
    "                                    performance_key = self.get_performance_key(document_key, user_name, 'keying', task_def_key) \n",
    "                                if source != 'queue_transform' and task_def_key == 'Verify_Hold_Type':\n",
    "                                    process_key = 12 # human check bad_image keyed_data not kpi                               \n",
    "                                elif source == 'queue_transform' and task_def_key.startswith('Type'):\n",
    "                                    process_key = 4 # 'machine save input keyed_data'\n",
    "                                elif source != 'queue_transform' and task_def_key.startswith('Proof'):\n",
    "                                    process_key = 5 # human qc input keyed_data' kpi\n",
    "                                    performance_key = self.get_performance_key(document_key, user_name, 'keying', task_def_key)                                  \n",
    "                                elif source == 'queue_transform' and task_def_key.startswith('Proof'):\n",
    "                                    process_key = 6 # 'machine save qc keyed_data'\n",
    "                                for field_name, field_value_dict in data_needed:\n",
    "                                    if field_name in col_ignores:\n",
    "                                        continue\n",
    "                                    _obj = FactDataExtractionModel(\n",
    "                                        document_key = document_key,\n",
    "                                        performance_key = performance_key,\n",
    "                                        ori_document_id = document_id,\n",
    "                                        project_id = self.project_id,\n",
    "                                        document_id = document_id,\n",
    "                                        doc_set_id =  doc_set_id,\n",
    "                                        record_id = record_id,\n",
    "                                        last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                        last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                        last_modified_timestamp = last_modified_utc_7,\n",
    "                                        user_name = user_name,\n",
    "                                        process_key = process_key,\n",
    "                                        field_name = field_name,\n",
    "                                        field_value = field_value_dict['text']\n",
    "                                    )\n",
    "                                    results.append(_obj)\n",
    "\n",
    "                        elif key == 'final_data':\n",
    "                            final_data = value[0]\n",
    "                            data_needed = final_data['data'][0].items()\n",
    "                            last_modified_utc_7 = final_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                            user_name = final_data['keyer']\n",
    "                            process_key = 10\n",
    "                            for field_name, field_value_dict in data_needed:\n",
    "                                if field_name in col_ignores:\n",
    "                                    continue\n",
    "                                _obj = FactDataExtractionModel(\n",
    "                                    document_key = document_key,\n",
    "                                    performance_key = None,\n",
    "                                    ori_document_id = document_id,\n",
    "                                    project_id = self.project_id,\n",
    "                                    document_id = document_id,\n",
    "                                    doc_set_id =  doc_set_id,\n",
    "                                    record_id = record_id,\n",
    "                                    last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                    last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                    last_modified_timestamp = last_modified_utc_7,\n",
    "                                    user_name = user_name,\n",
    "                                    process_key = process_key,\n",
    "                                    field_name = field_name,\n",
    "                                    field_value = field_value_dict['text']\n",
    "                                )\n",
    "                                results.append(_obj)\n",
    "\n",
    "                        elif key == 'qc_ed_data':\n",
    "                            qc_ed_data = value[0][0]\n",
    "                            if 'qc_fields_err' not in qc_ed_data.keys():\n",
    "                                continue\n",
    "                            qc_ed_data_err = qc_ed_data['qc_fields_err']\n",
    "                            data_needed = qc_ed_data_err[0].items()\n",
    "                            last_modified_utc_7 = qc_ed_data['createdtime'] + datetime.timedelta(hours = 7)\n",
    "                            user_name = qc_ed_data['keyer']\n",
    "                            process_key = 8\n",
    "                            performance_key = None\n",
    "                            performance_key = self.get_performance_key(document_key, user_name, 'qc', task_def_key) \n",
    "                            for field_name, field_value_dict in data_needed:\n",
    "                                if field_name in col_ignores:\n",
    "                                    continue\n",
    "                                _obj = FactDataExtractionModel(\n",
    "                                    document_key = document_key,\n",
    "                                    performance_key = performance_key,\n",
    "                                    ori_document_id = document_id,\n",
    "                                    project_id = self.project_id,\n",
    "                                    document_id = document_id,\n",
    "                                    doc_set_id =  doc_set_id,\n",
    "                                    record_id = record_id,\n",
    "                                    last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                                    last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                                    last_modified_timestamp = last_modified_utc_7,\n",
    "                                    user_name = user_name,\n",
    "                                    process_key = process_key,\n",
    "                                    field_name = field_name,\n",
    "                                    field_value = field_value_dict['text']\n",
    "                                )\n",
    "                                results.append(_obj)\n",
    "\n",
    "                        elif key == 'apr_ed_data':\n",
    "                            report.description = 'Not handle aprove qc data because not have sample data'\n",
    "                            \n",
    "            for data in data_trans:\n",
    "                trans_ignore = ['doc_id', 'doc_uri', 'fileName', 'fileName_Bad', 'filter_control', 'getBatchName', 'keyer', 'keyer_proof', 'keyer_type', 'FolderOutput', 'Image']\n",
    "                document_id = func.bson_object_to_string(data['doc_id'])\n",
    "                document_key = self.get_document_key_by_document_id(document_id)\n",
    "                ori_document_id = func.bson_object_to_string(data['_id'])\n",
    "                doc_set_id =  func.bson_object_to_string(data['doc_set_id'])\n",
    "                performance_key = None\n",
    "                records = data['records']\n",
    "                last_modified_utc_7 = data['last_modified'] + datetime.timedelta(hours = 7)\n",
    "                for i in range(len(records)):\n",
    "                    record = records[i]\n",
    "                    record_id = i + 1\n",
    "                    data_needed = record.items()\n",
    "                    for field_name, field_value in data_needed:\n",
    "                        if field_name in trans_ignore:\n",
    "                            continue\n",
    "                        _obj = FactDataExtractionModel(\n",
    "                            document_key = document_key,\n",
    "                            performance_key = performance_key,\n",
    "                            ori_document_id = ori_document_id,\n",
    "                            project_id = self.project_id,\n",
    "                            document_id = document_id,\n",
    "                            doc_set_id =  doc_set_id,\n",
    "                            record_id = record_id,\n",
    "                            last_modified_date_key = func.time_to_date_key(last_modified_utc_7),\n",
    "                            last_modified_time_key = func.time_to_time_key(last_modified_utc_7),\n",
    "                            last_modified_timestamp = last_modified_utc_7,\n",
    "                            user_name = None,\n",
    "                            process_key = 11,\n",
    "                            field_name = field_name,\n",
    "                            field_value = field_value\n",
    "                        )\n",
    "                        results.append(_obj)\n",
    "            self.db.create([item.__dict__ for item in results], self.schema, self.fact_data_extraction_table)\n",
    "            report.status_code = 'PASSED'\n",
    "        except Exception as e:\n",
    "            report.status_code = 'FAILED'\n",
    "            report.description = str(e)\n",
    "        finally:\n",
    "            report.total_time_run_second = time.time()-start_run\n",
    "            self.reports.append(report)\n",
    "            return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': ', The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/docs/2021-01-17.pickle '\n",
      "                'does not exist, The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/trans/2021-01-17.pickle '\n",
      "                'does not exist, The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/performance/2021-01-17.pickle '\n",
      "                'does not exist, The '\n",
      "                '/dags/data_warehouse_prod/backup/0473_200421_006_2020_10052_GDA_2020/field/2021-01-17.pickle '\n",
      "                'does not exist',\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 53, 56, 94219, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 95356,\n",
      " 'job_name': 'clean',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.0}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 53, 56, 95216, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 95356,\n",
      " 'job_name': 'backup_docs',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 2.5941267013549805}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 53, 58, 816341, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 95358,\n",
      " 'job_name': 'backup_trans',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.4415016174316406}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 53, 59, 284842, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 95359,\n",
      " 'job_name': 'backup_performance',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.270214319229126}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 53, 59, 558054, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 95359,\n",
      " 'job_name': 'backup_field',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 0.02199864387512207}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 53, 59, 581053, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 95359,\n",
      " 'job_name': 'dim_field',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 1.0396697521209717}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 54, 0, 621722, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 9540,\n",
      " 'job_name': 'fact_document',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 3.31493878364563}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 54, 4, 98661, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 9544,\n",
      " 'job_name': 'fact_performance',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 1.8223490715026855}\n",
      "{'description': None,\n",
      " 'executor_date_key': 2021119,\n",
      " 'executor_date_timestamp': datetime.datetime(2021, 1, 19, 9, 54, 5, 924039, tzinfo=tzfile('Asia/Ho_Chi_Minh')),\n",
      " 'executor_time_key': 9545,\n",
      " 'job_name': 'fact_data_extraction',\n",
      " 'project_id': '5e9e7ec598d753001b7efe6b',\n",
      " 'schedule_date_key': 20210101,\n",
      " 'schedule_time_key': 50000,\n",
      " 'status_code': 'PASSED',\n",
      " 'total_time_run_second': 568.1648716926575}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FactEtlReportModel' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-1b91bcc78bc7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfact_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfact_data_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mdb_connect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-570a409057b9>\u001b[0m in \u001b[0;36mreport\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;31m#==================================================================================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m         \u001b[0mreport_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreports\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfact_report_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;31m#==================================================================================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\github\\analytic\\dwh_analytic\\dags\\data_warehouse_prod\\functions\\report.py\u001b[0m in \u001b[0;36mupload_report\u001b[1;34m(reports, schema, table, db)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mupload_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDatabaseConnect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\github\\analytic\\dwh_analytic\\dags\\data_warehouse_prod\\functions\\db_connect.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, data, schema, table)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_exists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'append'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\analytic\\jupyter\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36m_json_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep, max_level)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrecord_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m             \u001b[1;31m# naive normalization, this is idempotent for flat records\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;31m# and potentially will inflate the data considerably for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\analytic\\jupyter\\lib\\site-packages\\pandas\\io\\json\\_normalize.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrecord_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m             \u001b[1;31m# naive normalization, this is idempotent for flat records\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m             \u001b[1;31m# and potentially will inflate the data considerably for\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FactEtlReportModel' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "db_connect = DatabaseConnect(uri = config.DWH_SQLALCHEMY_URI)\n",
    "executor = ProjectExecutor(\n",
    "    environment=config.ENVIRONMENT,\n",
    "    project_id=config.GDA_PROJECT_ID,\n",
    "    uri=config.ELROND_URI,\n",
    "    database_name=config.ELROND_DATABASE,\n",
    "    db = db_connect\n",
    ")\n",
    "pprint(executor.clean().__dict__)\n",
    "pprint(executor.backup_docs().__dict__)\n",
    "pprint(executor.backup_trans().__dict__)\n",
    "pprint(executor.backup_performance().__dict__)\n",
    "pprint(executor.backup_field().__dict__)\n",
    "pprint(executor.dim_field().__dict__)\n",
    "pprint(executor.fact_document().__dict__)\n",
    "pprint(executor.fact_performance().__dict__)\n",
    "pprint(executor.fact_data_extract().__dict__)\n",
    "pprint(executor.report())\n",
    "db_connect.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('xxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
